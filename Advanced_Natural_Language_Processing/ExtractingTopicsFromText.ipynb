{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Extracting Topics from Text\n\nIn this recipe, we are going to discuss how to identify topics from the document. Say, for example there is an online library with multiple department based on the kind of the book. As the new book comes in, you want to look at the unique keywords/topics and decide on which department this book might belong to and place it accordingly. In this kind of situation topic modelling would be handy"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "doc_complete=['I am learning NLP, it is very interesting and exiting it include machine learning and Deep learning',\n             'My father is data scientist and he is NLP expert',\n             'My sister has good exposure into android development']\ndoc_complete",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "['I am learning NLP, it is very interesting and exiting it include machine learning and Deep learning',\n 'My father is data scientist and he is NLP expert',\n 'My sister has good exposure into android development']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install gensim",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting gensim\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/db/677f0c8a1c49b44e7a999c2fdbcba576017c10d3d77d11c29ee3fa1b291e/gensim-3.8.1-cp35-cp35m-manylinux1_x86_64.whl (24.2MB)\n\u001b[K     |████████████████████████████████| 24.2MB 15kB/s  eta 0:00:01     |█████████████████▊              | 13.4MB 1.8MB/s eta 0:00:06     |████████████████████████▋       | 18.6MB 1.8MB/s eta 0:00:04     |████████████████████████████    | 21.1MB 2.3MB/s eta 0:00:02\n\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from gensim) (1.1.0)\nRequirement already satisfied: six>=1.5.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from gensim) (1.11.0)\nRequirement already satisfied: numpy>=1.11.3 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from gensim) (1.17.3)\nCollecting smart-open>=1.8.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n\u001b[K     |████████████████████████████████| 71kB 658kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: boto>=2.32 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from smart-open>=1.8.1->gensim) (2.42.0)\nRequirement already satisfied: requests in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from smart-open>=1.8.1->gensim) (2.14.2)\nRequirement already satisfied: boto3 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from smart-open>=1.8.1->gensim) (1.4.8)\nRequirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.1.13)\nRequirement already satisfied: botocore<1.9.0,>=1.8.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.8.50)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\nRequirement already satisfied: docutils>=0.10 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from botocore<1.9.0,>=1.8.0->boto3->smart-open>=1.8.1->gensim) (0.12)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from botocore<1.9.0,>=1.8.0->boto3->smart-open>=1.8.1->gensim) (2.8.1)\nBuilding wheels for collected packages: smart-open\n  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for smart-open: filename=smart_open-1.9.0-cp35-none-any.whl size=79335 sha256=2f18107b52fd17dda1203cb227cba7d5f54d164482f883879bc847de1deb2891\n  Stored in directory: /home/nbuser/.cache/pip/wheels/ab/10/93/5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\nSuccessfully built smart-open\nInstalling collected packages: smart-open, gensim\nSuccessfully installed gensim-3.8.1 smart-open-1.9.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nnltk.download('stopwords')\n!python -m textblob.download_corpora",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package brown to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/brown.zip.\n[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/nbuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package conll2000 to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/conll2000.zip.\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/movie_reviews.zip.\nFinished.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# preprocessing\nfrom nltk.corpus import stopwords\nfrom textblob import Word\nimport string\n\nstop = set(stopwords.words('english'))\nexclude = set(string.punctuation)\n\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    #print(stop_free)\n    punc_free = \"\".join([j for j in stop_free if j not in exclude])\n    #print(punc_free)\n    normalized = \" \".join([Word(k).lemmatize() for k in punc_free.split()])\n    #print(normalized)\n    return normalized\n    \ndoc_clean = [clean(doc).split() for doc in doc_complete]\ndoc_clean\n",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "[['learning',\n  'nlp',\n  'interesting',\n  'exiting',\n  'include',\n  'machine',\n  'learning',\n  'deep',\n  'learning'],\n ['father', 'data', 'scientist', 'nlp', 'expert'],\n ['sister', 'good', 'exposure', 'android', 'development']]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# preparing document term matrix\nimport gensim \nfrom gensim import corpora\n\ndictionary = corpora.Dictionary(doc_clean)  # creating the term dictionary of corpus where every unique term is assingned an index\nk = [k for k in dictionary.iteritems()]\nprint(k)\n# creating a list of documents(corpus) into Document-Term Matrix using dictionary prepared above\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\ndoc_term_matrix",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[(11, 'android'), (13, 'exposure'), (4, 'learning'), (2, 'include'), (8, 'expert'), (6, 'nlp'), (10, 'scientist'), (5, 'machine'), (3, 'interesting'), (14, 'good'), (1, 'exiting'), (15, 'sister'), (9, 'father'), (0, 'deep'), (7, 'data'), (12, 'development')]\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],\n [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "help(dictionary)",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Help on Dictionary in module gensim.corpora.dictionary object:\n\nclass Dictionary(gensim.utils.SaveLoad, collections.abc.Mapping)\n |  Dictionary encapsulates the mapping between normalized words and their integer ids.\n |  \n |  Notable instance attributes:\n |  \n |  Attributes\n |  ----------\n |  token2id : dict of (str, int)\n |      token -> tokenId.\n |  id2token : dict of (int, str)\n |      Reverse mapping for token2id, initialized in a lazy manner to save memory (not created until needed).\n |  cfs : dict of (int, int)\n |      Collection frequencies: token_id -> how many instances of this token are contained in the documents.\n |  dfs : dict of (int, int)\n |      Document frequencies: token_id -> how many documents contain this token.\n |  num_docs : int\n |      Number of documents processed.\n |  num_pos : int\n |      Total number of corpus positions (number of processed words).\n |  num_nnz : int\n |      Total number of non-zeroes in the BOW matrix (sum of the number of unique\n |      words per document over the entire corpus).\n |  \n |  Method resolution order:\n |      Dictionary\n |      gensim.utils.SaveLoad\n |      collections.abc.Mapping\n |      collections.abc.Sized\n |      collections.abc.Iterable\n |      collections.abc.Container\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getitem__(self, tokenid)\n |      Get the string token that corresponds to `tokenid`.\n |      \n |      Parameters\n |      ----------\n |      tokenid : int\n |          Id of token.\n |      \n |      Returns\n |      -------\n |      str\n |          Token corresponding to `tokenid`.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          If this Dictionary doesn't contain such `tokenid`.\n |  \n |  __init__(self, documents=None, prune_at=2000000)\n |      Parameters\n |      ----------\n |      documents : iterable of iterable of str, optional\n |          Documents to be used to initialize the mapping and collect corpus statistics.\n |      prune_at : int, optional\n |          Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n |          footprint, the correctness is not guaranteed.\n |          Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> texts = [['human', 'interface', 'computer']]\n |          >>> dct = Dictionary(texts)  # initialize a Dictionary\n |          >>> dct.add_documents([[\"cat\", \"say\", \"meow\"], [\"dog\"]])  # add more document (extend the vocabulary)\n |          >>> dct.doc2bow([\"dog\", \"computer\", \"non_existent_word\"])\n |          [(0, 1), (6, 1)]\n |  \n |  __iter__(self)\n |      Iterate over all tokens.\n |  \n |  __len__(self)\n |      Get number of stored tokens.\n |      \n |      Returns\n |      -------\n |      int\n |          Number of stored tokens.\n |  \n |  __str__(self)\n |      Return str(self).\n |  \n |  add_documents(self, documents, prune_at=2000000)\n |      Update dictionary from a collection of `documents`.\n |      \n |      Parameters\n |      ----------\n |      documents : iterable of iterable of str\n |          Input corpus. All tokens should be already **tokenized and normalized**.\n |      prune_at : int, optional\n |          Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n |          footprint, the correctness is not guaranteed.\n |          Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> corpus = [\"máma mele maso\".split(), \"ema má máma\".split()]\n |          >>> dct = Dictionary(corpus)\n |          >>> len(dct)\n |          5\n |          >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\n |          >>> len(dct)\n |          10\n |  \n |  compactify(self)\n |      Assign new word ids to all words, shrinking any gaps.\n |  \n |  doc2bow(self, document, allow_update=False, return_missing=False)\n |      Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\n |      \n |      Parameters\n |      ----------\n |      document : list of str\n |          Input document.\n |      allow_update : bool, optional\n |          Update self, by adding new tokens from `document` and updating internal corpus statistics.\n |      return_missing : bool, optional\n |          Return missing tokens (tokens present in `document` but not in self) with frequencies?\n |      \n |      Return\n |      ------\n |      list of (int, int)\n |          BoW representation of `document`.\n |      list of (int, int), dict of (str, int)\n |          If `return_missing` is True, return BoW representation of `document` + dictionary with missing\n |          tokens and their frequencies.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>> dct = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n |          >>> dct.doc2bow([\"this\", \"is\", \"máma\"])\n |          [(2, 1)]\n |          >>> dct.doc2bow([\"this\", \"is\", \"máma\"], return_missing=True)\n |          ([(2, 1)], {u'this': 1, u'is': 1})\n |  \n |  doc2idx(self, document, unknown_word_index=-1)\n |      Convert `document` (a list of words) into a list of indexes = list of `token_id`.\n |      Replace all unknown words i.e, words not in the dictionary with the index as set via `unknown_word_index`.\n |      \n |      Parameters\n |      ----------\n |      document : list of str\n |          Input document\n |      unknown_word_index : int, optional\n |          Index to use for words not in the dictionary.\n |      \n |      Returns\n |      -------\n |      list of int\n |          Token ids for tokens in `document`, in the same order.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> corpus = [[\"a\", \"a\", \"b\"], [\"a\", \"c\"]]\n |          >>> dct = Dictionary(corpus)\n |          >>> dct.doc2idx([\"a\", \"a\", \"c\", \"not_in_dictionary\", \"c\"])\n |          [0, 0, 2, -1, 2]\n |  \n |  filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000, keep_tokens=None)\n |      Filter out tokens in the dictionary by their frequency.\n |      \n |      Parameters\n |      ----------\n |      no_below : int, optional\n |          Keep tokens which are contained in at least `no_below` documents.\n |      no_above : float, optional\n |          Keep tokens which are contained in no more than `no_above` documents\n |          (fraction of total corpus size, not an absolute number).\n |      keep_n : int, optional\n |          Keep only the first `keep_n` most frequent tokens.\n |      keep_tokens : iterable of str\n |          Iterable of tokens that **must** stay in dictionary after filtering.\n |      \n |      Notes\n |      -----\n |      This removes all tokens in the dictionary that are:\n |      \n |      #. Less frequent than `no_below` documents (absolute number, e.g. `5`) or \n |      \n |      #. More frequent than `no_above` documents (fraction of the total corpus size, e.g. `0.3`).\n |      #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `keep_n=None`).\n |      \n |      After the pruning, resulting gaps in word ids are shrunk.\n |      Due to this gap shrinking, **the same word may have a different word id before and after the call\n |      to this function!**\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n |          >>> dct = Dictionary(corpus)\n |          >>> len(dct)\n |          5\n |          >>> dct.filter_extremes(no_below=1, no_above=0.5, keep_n=1)\n |          >>> len(dct)\n |          1\n |  \n |  filter_n_most_frequent(self, remove_n)\n |      Filter out the 'remove_n' most frequent tokens that appear in the documents.\n |      \n |      Parameters\n |      ----------\n |      remove_n : int\n |          Number of the most frequent tokens that will be removed.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n |          >>> dct = Dictionary(corpus)\n |          >>> len(dct)\n |          5\n |          >>> dct.filter_n_most_frequent(2)\n |          >>> len(dct)\n |          3\n |  \n |  filter_tokens(self, bad_ids=None, good_ids=None)\n |      Remove the selected `bad_ids` tokens from :class:`~gensim.corpora.dictionary.Dictionary`.\n |      \n |      Alternatively, keep selected `good_ids` in :class:`~gensim.corpora.dictionary.Dictionary` and remove the rest.\n |      \n |      Parameters\n |      ----------\n |      bad_ids : iterable of int, optional\n |          Collection of word ids to be removed.\n |      good_ids : collection of int, optional\n |          Keep selected collection of word ids and remove the rest.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n |          >>> dct = Dictionary(corpus)\n |          >>> 'ema' in dct.token2id\n |          True\n |          >>> dct.filter_tokens(bad_ids=[dct.token2id['ema']])\n |          >>> 'ema' in dct.token2id\n |          False\n |          >>> len(dct)\n |          4\n |          >>> dct.filter_tokens(good_ids=[dct.token2id['maso']])\n |          >>> len(dct)\n |          1\n |  \n |  iteritems(self)\n |  \n |  iterkeys = __iter__(self)\n |  \n |  itervalues(self)\n |  \n |  keys(self)\n |      Get all stored ids.\n |      \n |      Returns\n |      -------\n |      list of int\n |          List of all token ids.\n |  \n |  merge_with(self, other)\n |      Merge another dictionary into this dictionary, mapping the same tokens to the same ids\n |      and new tokens to new ids.\n |      \n |      Notes\n |      -----\n |      The purpose is to merge two corpora created using two different dictionaries: `self` and `other`.\n |      `other` can be any id=>word mapping (a dict, a Dictionary object, ...).\n |      \n |      Return a transformation object which, when accessed as `result[doc_from_other_corpus]`, will convert documents\n |      from a corpus built using the `other` dictionary into a document using the new, merged dictionary.\n |      \n |      Parameters\n |      ----------\n |      other : {dict, :class:`~gensim.corpora.dictionary.Dictionary`}\n |          Other dictionary.\n |      \n |      Return\n |      ------\n |      :class:`gensim.models.VocabTransform`\n |          Transformation object.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> corpus_1, corpus_2 = [[\"a\", \"b\", \"c\"]], [[\"a\", \"f\", \"f\"]]\n |          >>> dct_1, dct_2 = Dictionary(corpus_1), Dictionary(corpus_2)\n |          >>> dct_1.doc2bow(corpus_2[0])\n |          [(0, 1)]\n |          >>> transformer = dct_1.merge_with(dct_2)\n |          >>> dct_1.doc2bow(corpus_2[0])\n |          [(0, 1), (3, 2)]\n |  \n |  patch_with_special_tokens(self, special_token_dict)\n |      Patch token2id and id2token using a dictionary of special tokens.\n |      \n |      \n |      **Usecase:** when doing sequence modeling (e.g. named entity recognition), one may  want to specify\n |      special tokens that behave differently than others.\n |      One example is the \"unknown\" token, and another is the padding token.\n |      It is usual to set the padding token to have index `0`, and patching the dictionary with `{'<PAD>': 0}`\n |      would be one way to specify this.\n |      \n |      Parameters\n |      ----------\n |      special_token_dict : dict of (str, int)\n |          dict containing the special tokens as keys and their wanted indices as values.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n |          >>> dct = Dictionary(corpus)\n |          >>>\n |          >>> special_tokens = {'pad': 0, 'space': 1}\n |          >>> print(dct.token2id)\n |          {'maso': 0, 'mele': 1, 'máma': 2, 'ema': 3, 'má': 4}\n |          >>>\n |          >>> dct.patch_with_special_tokens(special_tokens)\n |          >>> print(dct.token2id)\n |          {'maso': 6, 'mele': 7, 'máma': 2, 'ema': 3, 'má': 4, 'pad': 0, 'space': 1}\n |  \n |  save_as_text(self, fname, sort_by_word=True)\n |      Save :class:`~gensim.corpora.dictionary.Dictionary` to a text file.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          Path to output file.\n |      sort_by_word : bool, optional\n |          Sort words in lexicographical order before writing them out?\n |      \n |      Notes\n |      -----\n |      Format::\n |      \n |          num_docs\n |          id_1[TAB]word_1[TAB]document_frequency_1[NEWLINE]\n |          id_2[TAB]word_2[TAB]document_frequency_2[NEWLINE]\n |          ....\n |          id_k[TAB]word_k[TAB]document_frequency_k[NEWLINE]\n |      \n |      This text format is great for corpus inspection and debugging. As plaintext, it's also easily portable\n |      to other tools and frameworks. For better performance and to store the entire object state,\n |      including collected corpus statistics, use :meth:`~gensim.corpora.dictionary.Dictionary.save` and\n |      :meth:`~gensim.corpora.dictionary.Dictionary.load` instead.\n |      \n |      See Also\n |      --------\n |      :meth:`~gensim.corpora.dictionary.Dictionary.load_from_text`\n |          Load :class:`~gensim.corpora.dictionary.Dictionary` from text file.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>> from gensim.test.utils import get_tmpfile\n |          >>>\n |          >>> tmp_fname = get_tmpfile(\"dictionary\")\n |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n |          >>>\n |          >>> dct = Dictionary(corpus)\n |          >>> dct.save_as_text(tmp_fname)\n |          >>>\n |          >>> loaded_dct = Dictionary.load_from_text(tmp_fname)\n |          >>> assert dct.token2id == loaded_dct.token2id\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  from_corpus(corpus, id2word=None)\n |      Create :class:`~gensim.corpora.dictionary.Dictionary` from an existing corpus.\n |      \n |      Parameters\n |      ----------\n |      corpus : iterable of iterable of (int, number)\n |          Corpus in BoW format.\n |      id2word : dict of (int, object)\n |          Mapping id -> word. If None, the mapping `id2word[word_id] = str(word_id)` will be used.\n |      \n |      Notes\n |      -----\n |      This can be useful if you only have a term-document BOW matrix (represented by `corpus`), but not the original\n |      text corpus. This method will scan the term-document count matrix for all word ids that appear in it,\n |      then construct :class:`~gensim.corpora.dictionary.Dictionary` which maps each `word_id -> id2word[word_id]`.\n |      `id2word` is an optional dictionary that maps the `word_id` to a token.\n |      In case `id2word` isn't specified the mapping `id2word[word_id] = str(word_id)` will be used.\n |      \n |      Returns\n |      -------\n |      :class:`~gensim.corpora.dictionary.Dictionary`\n |          Inferred dictionary from corpus.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>>\n |          >>> corpus = [[(1, 1.0)], [], [(0, 5.0), (2, 1.0)], []]\n |          >>> dct = Dictionary.from_corpus(corpus)\n |          >>> len(dct)\n |          3\n |  \n |  from_documents(documents)\n |      Create :class:`~gensim.corpora.dictionary.Dictionary` from `documents`.\n |      \n |      Equivalent to `Dictionary(documents=documents)`.\n |      \n |      Parameters\n |      ----------\n |      documents : iterable of iterable of str\n |          Input corpus.\n |      \n |      Returns\n |      -------\n |      :class:`~gensim.corpora.dictionary.Dictionary`\n |          Dictionary initialized from `documents`.\n |  \n |  load_from_text(fname)\n |      Load a previously stored :class:`~gensim.corpora.dictionary.Dictionary` from a text file.\n |      \n |      Mirror function to :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`.\n |      \n |      Parameters\n |      ----------\n |      fname: str\n |          Path to a file produced by :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`.\n |      \n |      See Also\n |      --------\n |      :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`\n |          Save :class:`~gensim.corpora.dictionary.Dictionary` to text file.\n |      \n |      Examples\n |      --------\n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.corpora import Dictionary\n |          >>> from gensim.test.utils import get_tmpfile\n |          >>>\n |          >>> tmp_fname = get_tmpfile(\"dictionary\")\n |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n |          >>>\n |          >>> dct = Dictionary(corpus)\n |          >>> dct.save_as_text(tmp_fname)\n |          >>>\n |          >>> loaded_dct = Dictionary.load_from_text(tmp_fname)\n |          >>> assert dct.token2id == loaded_dct.token2id\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from gensim.utils.SaveLoad:\n |  \n |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n |      Save the object to a file.\n |      \n |      Parameters\n |      ----------\n |      fname_or_handle : str or file-like\n |          Path to output file or already opened file-like object. If the object is a file handle,\n |          no special array handling will be performed, all attributes will be saved to the same file.\n |      separately : list of str or None, optional\n |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n |          them into separate files. This prevent memory errors for large objects, and also allows\n |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n |          loading and sharing the large arrays in RAM between multiple processes.\n |      \n |          If list of str: store these attributes into separate files. The automated size check\n |          is not performed in this case.\n |      sep_limit : int, optional\n |          Don't store arrays smaller than this separately. In bytes.\n |      ignore : frozenset of str, optional\n |          Attributes that shouldn't be stored at all.\n |      pickle_protocol : int, optional\n |          Protocol number for pickle.\n |      \n |      See Also\n |      --------\n |      :meth:`~gensim.utils.SaveLoad.load`\n |          Load object from file.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from gensim.utils.SaveLoad:\n |  \n |  load(fname, mmap=None) from abc.ABCMeta\n |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          Path to file that contains needed object.\n |      mmap : str, optional\n |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n |          via mmap (shared memory) using `mmap='r'.\n |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n |      \n |      See Also\n |      --------\n |      :meth:`~gensim.utils.SaveLoad.save`\n |          Save object to file.\n |      \n |      Returns\n |      -------\n |      object\n |          Object loaded from `fname`.\n |      \n |      Raises\n |      ------\n |      AttributeError\n |          When called on an object instance instead of class (this is a class method).\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from gensim.utils.SaveLoad:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from collections.abc.Mapping:\n |  \n |  __contains__(self, key)\n |  \n |  __eq__(self, other)\n |      Return self==value.\n |  \n |  get(self, key, default=None)\n |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n |  \n |  items(self)\n |      D.items() -> a set-like object providing a view on D's items\n |  \n |  values(self)\n |      D.values() -> an object providing a view on D's values\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from collections.abc.Mapping:\n |  \n |  __hash__ = None\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from collections.abc.Sized:\n |  \n |  __subclasshook__(C) from abc.ABCMeta\n |      Abstract classes can override this to customize issubclass().\n |      \n |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n |      It should return True, False or NotImplemented.  If it returns\n |      NotImplemented, the normal algorithm is used.  Otherwise, it\n |      overrides the normal algorithm (and the outcome is cached).\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Lda = gensim.models.ldamodel.LdaModel # creating the object for LDA model using gensim library\n\nldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50) # Running and Training LDA model on the document term matrix for 3 topics.\n\nprint(ldamodel.print_topics())",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[(0, '0.129*\"sister\" + 0.129*\"good\" + 0.129*\"exposure\" + 0.129*\"development\" + 0.129*\"android\" + 0.032*\"nlp\" + 0.032*\"father\" + 0.032*\"scientist\" + 0.032*\"data\" + 0.032*\"expert\"'), (1, '0.233*\"learning\" + 0.093*\"deep\" + 0.093*\"include\" + 0.093*\"interesting\" + 0.093*\"machine\" + 0.093*\"exiting\" + 0.093*\"nlp\" + 0.023*\"scientist\" + 0.023*\"data\" + 0.023*\"father\"'), (2, '0.129*\"nlp\" + 0.129*\"father\" + 0.129*\"data\" + 0.129*\"scientist\" + 0.129*\"expert\" + 0.032*\"exposure\" + 0.032*\"good\" + 0.032*\"development\" + 0.032*\"android\" + 0.032*\"sister\"')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "help(ldamodel)",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Help on LdaModel in module gensim.models.ldamodel object:\n\nclass LdaModel(gensim.interfaces.TransformationABC, gensim.models.basemodel.BaseTopicModel)\n |  Train and use Online Latent Dirichlet Allocation (OLDA) models as presented in\n |  `Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n |  \n |  Examples\n |  -------\n |  Initialize a model using a Gensim corpus\n |  \n |  .. sourcecode:: pycon\n |  \n |      >>> from gensim.test.utils import common_corpus\n |      >>>\n |      >>> lda = LdaModel(common_corpus, num_topics=10)\n |  \n |  You can then infer topic distributions on new, unseen documents.\n |  \n |  .. sourcecode:: pycon\n |  \n |      >>> doc_bow = [(1, 0.3), (2, 0.1), (0, 0.09)]\n |      >>> doc_lda = lda[doc_bow]\n |  \n |  The model can be updated (trained) with new documents.\n |  \n |  .. sourcecode:: pycon\n |  \n |      >>> # In practice (corpus =/= initial training corpus), but we use the same here for simplicity.\n |      >>> other_corpus = common_corpus\n |      >>>\n |      >>> lda.update(other_corpus)\n |  \n |  Model persistency is achieved through :meth:`~gensim.models.ldamodel.LdaModel.load` and\n |  :meth:`~gensim.models.ldamodel.LdaModel.save` methods.\n |  \n |  Method resolution order:\n |      LdaModel\n |      gensim.interfaces.TransformationABC\n |      gensim.utils.SaveLoad\n |      gensim.models.basemodel.BaseTopicModel\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getitem__(self, bow, eps=None)\n |      Get the topic distribution for the given document.\n |      \n |      Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\n |      Uses the model's current state (set using constructor arguments) to fill in the additional arguments of the\n |      wrapper method.\n |      \n |      Parameters\n |      ---------\n |      bow : list of (int, float)\n |          The document in BOW format.\n |      eps : float, optional\n |          Topics with an assigned probability lower than this threshold will be discarded.\n |      \n |      Returns\n |      -------\n |      list of (int, float)\n |          Topic distribution for the given document. Each topic is represented as a pair of its ID and the probability\n |          assigned to it.\n |  \n |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<class 'numpy.float32'>)\n |      Parameters\n |      ----------\n |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`).\n |          If not given, the model is left untrained (presumably because you want to call\n |          :meth:`~gensim.models.ldamodel.LdaModel.update` manually).\n |      num_topics : int, optional\n |          The number of requested latent topics to be extracted from the training corpus.\n |      id2word : {dict of (int, str), :class:`gensim.corpora.dictionary.Dictionary`}\n |          Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n |          debugging and topic printing.\n |      distributed : bool, optional\n |          Whether distributed computing should be used to accelerate training.\n |      chunksize :  int, optional\n |          Number of documents to be used in each training chunk.\n |      passes : int, optional\n |          Number of passes through the corpus during training.\n |      update_every : int, optional\n |          Number of documents to be iterated through for each update.\n |          Set to 0 for batch learning, > 1 for online iterative learning.\n |      alpha : {numpy.ndarray, str}, optional\n |          Can be set to an 1D array of length equal to the number of expected topics that expresses\n |          our a-priori belief for the each topics' probability.\n |          Alternatively default prior selecting strategies can be employed by supplying a string:\n |      \n |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n |              * 'auto': Learns an asymmetric prior from the corpus (not available if `distributed==True`).\n |      eta : {float, np.array, str}, optional\n |          A-priori belief on word probability, this can be:\n |      \n |              * scalar for a symmetric prior over topic/word probability,\n |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n |              * the string 'auto' to learn the asymmetric prior from the data.\n |      decay : float, optional\n |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n |          when each new document is examined. Corresponds to Kappa from\n |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n |      offset : float, optional\n |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n |      eval_every : int, optional\n |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n |      iterations : int, optional\n |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n |      gamma_threshold : float, optional\n |          Minimum change in the value of the gamma parameters to continue iterating.\n |      minimum_probability : float, optional\n |          Topics with a probability lower than this threshold will be filtered out.\n |      random_state : {np.random.RandomState, int}, optional\n |          Either a randomState object or a seed to generate one. Useful for reproducibility.\n |      ns_conf : dict of (str, object), optional\n |          Key word parameters propagated to :func:`gensim.utils.getNS` to get a Pyro4 Nameserved.\n |          Only used if `distributed` is set to True.\n |      minimum_phi_value : float, optional\n |          if `per_word_topics` is True, this represents a lower bound on the term probabilities.\n |      per_word_topics : bool\n |          If True, the model also computes a list of topics, sorted in descending order of most likely topics for\n |          each word, along with their phi values multiplied by the feature length (i.e. word count).\n |      callbacks : list of :class:`~gensim.models.callbacks.Callback`\n |          Metric callbacks to log and visualize evaluation metrics of the model during training.\n |      dtype : {numpy.float16, numpy.float32, numpy.float64}, optional\n |          Data-type to use during calculations inside model. All inputs are also converted.\n |  \n |  __str__(self)\n |      Get a string representation of the current object.\n |      \n |      Returns\n |      -------\n |      str\n |          Human readable representation of the most important model parameters.\n |  \n |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n |      Estimate the variational bound of documents from the corpus as E_q[log p(corpus)] - E_q[log q(corpus)].\n |      \n |      Parameters\n |      ----------\n |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`) used to estimate the\n |          variational bounds.\n |      gamma : numpy.ndarray, optional\n |          Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n |      subsample_ratio : float, optional\n |          Percentage of the whole corpus represented by the passed `corpus` argument (in case this was a sample).\n |          Set to 1.0 if the whole corpus was passed.This is used as a multiplicative factor to scale the likelihood\n |          appropriately.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          The variational bound score calculated for each document.\n |  \n |  clear(self)\n |      Clear the model's state to free some memory. Used in the distributed implementation.\n |  \n |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, diagonal=False, annotation=True, normed=True)\n |      Calculate the difference in topic distributions between two models: `self` and `other`.\n |      \n |      Parameters\n |      ----------\n |      other : :class:`~gensim.models.ldamodel.LdaModel`\n |          The model which will be compared against the current object.\n |      distance : {'kullback_leibler', 'hellinger', 'jaccard', 'jensen_shannon'}\n |          The distance metric to calculate the difference with.\n |      num_words : int, optional\n |          The number of most relevant words used if `distance == 'jaccard'`. Also used for annotating topics.\n |      n_ann_terms : int, optional\n |          Max number of words in intersection/symmetric difference between topics. Used for annotation.\n |      diagonal : bool, optional\n |          Whether we need the difference between identical topics (the diagonal of the difference matrix).\n |      annotation : bool, optional\n |          Whether the intersection or difference of words between two topics should be returned.\n |      normed : bool, optional\n |          Whether the matrix should be normalized or not.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          A difference matrix. Each element corresponds to the difference between the two topics,\n |          shape (`self.num_topics`, `other.num_topics`)\n |      numpy.ndarray, optional\n |          Annotation matrix where for each pair we include the word from the intersection of the two topics,\n |          and the word from the symmetric difference of the two topics. Only included if `annotation == True`.\n |          Shape (`self.num_topics`, `other_model.num_topics`, 2).\n |      \n |      Examples\n |      --------\n |      Get the differences between each pair of topics inferred by two models\n |      \n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.models.ldamulticore import LdaMulticore\n |          >>> from gensim.test.utils import datapath\n |          >>>\n |          >>> m1 = LdaMulticore.load(datapath(\"lda_3_0_1_model\"))\n |          >>> m2 = LdaMulticore.load(datapath(\"ldamodel_python_3_5\"))\n |          >>> mdiff, annotation = m1.diff(m2)\n |          >>> topic_diff = mdiff  # get matrix with difference for each topic pair from `m1` and `m2`\n |  \n |  do_estep(self, chunk, state=None)\n |      Perform inference on a chunk of documents, and accumulate the collected sufficient statistics.\n |      \n |      Parameters\n |      ----------\n |      chunk : {list of list of (int, float), scipy.sparse.csc}\n |          The corpus chunk on which the inference step will be performed.\n |      state : :class:`~gensim.models.ldamodel.LdaState`, optional\n |          The state to be updated with the newly accumulated sufficient statistics. If none, the models\n |          `self.state` is updated.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          Gamma parameters controlling the topic weights, shape (`len(chunk)`, `self.num_topics`).\n |  \n |  do_mstep(self, rho, other, extra_pass=False)\n |      Maximization step: use linear interpolation between the existing topics and\n |      collected sufficient statistics in `other` to update the topics.\n |      \n |      Parameters\n |      ----------\n |      rho : float\n |          Learning rate.\n |      other : :class:`~gensim.models.ldamodel.LdaModel`\n |          The model whose sufficient statistics will be used to update the topics.\n |      extra_pass : bool, optional\n |          Whether this step required an additional pass over the corpus.\n |  \n |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n |      Get the topic distribution for the given document.\n |      \n |      Parameters\n |      ----------\n |      bow : corpus : list of (int, float)\n |          The document in BOW format.\n |      minimum_probability : float\n |          Topics with an assigned probability lower than this threshold will be discarded.\n |      minimum_phi_value : float\n |          If `per_word_topics` is True, this represents a lower bound on the term probabilities that are included.\n |           If set to None, a value of 1e-8 is used to prevent 0s.\n |      per_word_topics : bool\n |          If True, this function will also return two extra lists as explained in the \"Returns\" section.\n |      \n |      Returns\n |      -------\n |      list of (int, float)\n |          Topic distribution for the whole document. Each element in the list is a pair of a topic's id, and\n |          the probability that was assigned to it.\n |      list of (int, list of (int, float), optional\n |          Most probable topics per word. Each element in the list is a pair of a word's id, and a list of\n |          topics sorted by their relevance to this word. Only returned if `per_word_topics` was set to True.\n |      list of (int, list of float), optional\n |          Phi relevance values, multiplied by the feature length, for each word-topic combination.\n |          Each element in the list is a pair of a word's id and a list of the phi values between this word and\n |          each topic. Only returned if `per_word_topics` was set to True.\n |  \n |  get_term_topics(self, word_id, minimum_probability=None)\n |      Get the most relevant topics to the given word.\n |      \n |      Parameters\n |      ----------\n |      word_id : int\n |          The word for which the topic distribution will be computed.\n |      minimum_probability : float, optional\n |          Topics with an assigned probability below this threshold will be discarded.\n |      \n |      Returns\n |      -------\n |      list of (int, float)\n |          The relevant topics represented as pairs of their ID and their assigned probability, sorted\n |          by relevance to the given word.\n |  \n |  get_topic_terms(self, topicid, topn=10)\n |      Get the representation for a single topic. Words the integer IDs, in constrast to\n |      :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\n |      \n |      Parameters\n |      ----------\n |      topicid : int\n |          The ID of the topic to be returned\n |      topn : int, optional\n |          Number of the most significant words that are associated with the topic.\n |      \n |      Returns\n |      -------\n |      list of (int, float)\n |          Word ID - probability pairs for the most relevant words generated by the topic.\n |  \n |  get_topics(self)\n |      Get the term-topic matrix learned during inference.\n |      \n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          The probability for each word in each topic, shape (`num_topics`, `vocabulary_size`).\n |  \n |  inference(self, chunk, collect_sstats=False)\n |      Given a chunk of sparse document vectors, estimate gamma (parameters controlling the topic weights)\n |      for each document in the chunk.\n |      \n |      This function does not modify the model The whole input chunk of document is assumed to fit in RAM;\n |      chunking of a large corpus must be done earlier in the pipeline. Avoids computing the `phi` variational\n |      parameter directly using the optimization presented in\n |      `Lee, Seung: Algorithms for non-negative matrix factorization\"\n |      <https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf>`_.\n |      \n |      Parameters\n |      ----------\n |      chunk : {list of list of (int, float), scipy.sparse.csc}\n |          The corpus chunk on which the inference step will be performed.\n |      collect_sstats : bool, optional\n |          If set to True, also collect (and return) sufficient statistics needed to update the model's topic-word\n |          distributions.\n |      \n |      Returns\n |      -------\n |      (numpy.ndarray, {numpy.ndarray, None})\n |          The first element is always returned and it corresponds to the states gamma matrix. The second element is\n |          only returned if `collect_sstats` == True and corresponds to the sufficient statistics for the M step.\n |  \n |  init_dir_prior(self, prior, name)\n |      Initialize priors for the Dirichlet distribution.\n |      \n |      Parameters\n |      ----------\n |      prior : {str, list of float, numpy.ndarray of float, float}\n |          A-priori belief on word probability. If `name` == 'eta' then the prior can be:\n |      \n |              * scalar for a symmetric prior over topic/word probability,\n |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n |              * the string 'auto' to learn the asymmetric prior from the data.\n |      \n |          If `name` == 'alpha', then the prior can be:\n |      \n |              * an 1D array of length equal to the number of expected topics,\n |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n |              * 'auto': Learns an asymmetric prior from the corpus.\n |      name : {'alpha', 'eta'}\n |          Whether the `prior` is parameterized by the alpha vector (1 parameter per topic)\n |          or by the eta (1 parameter per unique term in the vocabulary).\n |  \n |  log_perplexity(self, chunk, total_docs=None)\n |      Calculate and return per-word likelihood bound, using a chunk of documents as evaluation corpus.\n |      \n |      Also output the calculated statistics, including the perplexity=2^(-bound), to log at INFO level.\n |      \n |      Parameters\n |      ----------\n |      chunk : {list of list of (int, float), scipy.sparse.csc}\n |          The corpus chunk on which the inference step will be performed.\n |      total_docs : int, optional\n |          Number of docs used for evaluation of the perplexity.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          The variational bound score calculated for each word.\n |  \n |  save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs)\n |      Save the model to a file.\n |      \n |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n |      \n |      Notes\n |      -----\n |      If you intend to use models across Python 2/3 versions there are a few things to\n |      keep in mind:\n |      \n |        1. The pickled Python dictionaries will not work across Python versions\n |        2. The `save` method does not automatically save all numpy arrays separately, only\n |           those ones that exceed `sep_limit` set in :meth:`~gensim.utils.SaveLoad.save`. The main\n |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n |      \n |      Please refer to the `wiki recipes section\n |      <https://github.com/RaRe-Technologies/gensim/wiki/\n |      Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2>`_\n |      for an example on how to work around these issues.\n |      \n |      See Also\n |      --------\n |      :meth:`~gensim.models.ldamodel.LdaModel.load`\n |          Load model.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          Path to the system file where the model will be persisted.\n |      ignore : tuple of str, optional\n |          The named attributes in the tuple will be left out of the pickled model. The reason why\n |          the internal `state` is ignored by default is that it uses its own serialisation rather than the one\n |          provided by this method.\n |      separately : {list of str, None}, optional\n |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n |          them into separate files. This avoids pickle memory errors and allows `mmap`'ing large arrays\n |          back on load efficiently. If list of str - this attributes will be stored in separate files,\n |          the automatic check is not performed in this case.\n |      *args\n |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n |      **kwargs\n |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n |  \n |  show_topic(self, topicid, topn=10)\n |      Get the representation for a single topic. Words here are the actual strings, in constrast to\n |      :meth:`~gensim.models.ldamodel.LdaModel.get_topic_terms` that represents words by their vocabulary ID.\n |      \n |      Parameters\n |      ----------\n |      topicid : int\n |          The ID of the topic to be returned\n |      topn : int, optional\n |          Number of the most significant words that are associated with the topic.\n |      \n |      Returns\n |      -------\n |      list of (str, float)\n |          Word - probability pairs for the most relevant words generated by the topic.\n |  \n |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n |      Get a representation for selected topics.\n |      \n |      Parameters\n |      ----------\n |      num_topics : int, optional\n |          Number of topics to be returned. Unlike LSA, there is no natural ordering between the topics in LDA.\n |          The returned topics subset of all topics is therefore arbitrary and may change between two LDA\n |          training runs.\n |      num_words : int, optional\n |          Number of words to be presented for each topic. These will be the most relevant words (assigned the highest\n |          probability for each topic).\n |      log : bool, optional\n |          Whether the output is also logged, besides being returned.\n |      formatted : bool, optional\n |          Whether the topic representations should be formatted as strings. If False, they are returned as\n |          2 tuples of (word, probability).\n |      \n |      Returns\n |      -------\n |      list of {str, tuple of (str, float)}\n |          a list of topics, each represented either as a string (when `formatted` == True) or word-probability\n |          pairs.\n |  \n |  sync_state(self, current_Elogbeta=None)\n |      Propagate the states topic probabilities to the inner object's attribute.\n |      \n |      Parameters\n |      ----------\n |      current_Elogbeta: numpy.ndarray\n |          Posterior probabilities for each topic, optional.\n |          If omitted, it will get Elogbeta from state.\n |  \n |  top_topics(self, corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1)\n |      Get the topics with the highest coherence score the coherence for each topic.\n |      \n |      Parameters\n |      ----------\n |      corpus : iterable of list of (int, float), optional\n |          Corpus in BoW format.\n |      texts : list of list of str, optional\n |          Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`)\n |          probability estimator .\n |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n |          Gensim dictionary mapping of id word to create corpus.\n |          If `model.id2word` is present, this is not needed. If both are provided, passed `dictionary` will be used.\n |      window_size : int, optional\n |          Is the size of the window to be used for coherence measures using boolean sliding window as their\n |          probability estimator. For 'u_mass' this doesn't matter.\n |          If None - the default window sizes are used which are: 'c_v' - 110, 'c_uci' - 10, 'c_npmi' - 10.\n |      coherence : {'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional\n |          Coherence measure to be used.\n |          Fastest method - 'u_mass', 'c_uci' also known as `c_pmi`.\n |          For 'u_mass' corpus should be provided, if texts is provided, it will be converted to corpus\n |          using the dictionary. For 'c_v', 'c_uci' and 'c_npmi' `texts` should be provided (`corpus` isn't needed)\n |      topn : int, optional\n |          Integer corresponding to the number of top words to be extracted from each topic.\n |      processes : int, optional\n |          Number of processes to use for probability estimation phase, any value less than 1 will be interpreted as\n |          num_cpus - 1.\n |      \n |      Returns\n |      -------\n |      list of (list of (int, str), float)\n |          Each element in the list is a pair of a topic representation and its coherence score. Topic representations\n |          are distributions of words, represented as a list of pairs of word IDs and their probabilities.\n |  \n |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n |      Train the model with new documents, by EM-iterating over the corpus until the topics converge, or until\n |      the maximum number of allowed iterations is reached. `corpus` must be an iterable.\n |      \n |      In distributed mode, the E step is distributed over a cluster of machines.\n |      \n |      Notes\n |      -----\n |      This update also supports updating an already trained model with new documents; the two models are then merged\n |      in proportion to the number of old vs. new documents. This feature is still experimental for non-stationary\n |      input streams. For stationary input (no topic drift in new documents), on the other hand, this equals the\n |      online update of `Matthew D. Hoffman, David M. Blei, Francis Bach:\n |      \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n |      and is guaranteed to converge for any `decay` in (0.5, 1.0). Additionally, for smaller corpus sizes, an\n |      increasing `offset` may be beneficial (see Table 1 in the same paper).\n |      \n |      Parameters\n |      ----------\n |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`) used to update the\n |          model.\n |      chunksize :  int, optional\n |          Number of documents to be used in each training chunk.\n |      decay : float, optional\n |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n |          when each new document is examined. Corresponds to Kappa from\n |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n |      offset : float, optional\n |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n |      passes : int, optional\n |          Number of passes through the corpus during training.\n |      update_every : int, optional\n |          Number of documents to be iterated through for each update.\n |          Set to 0 for batch learning, > 1 for online iterative learning.\n |      eval_every : int, optional\n |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n |      iterations : int, optional\n |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n |      gamma_threshold : float, optional\n |          Minimum change in the value of the gamma parameters to continue iterating.\n |      chunks_as_numpy : bool, optional\n |          Whether each chunk passed to the inference step should be a numpy.ndarray or not. Numpy can in some settings\n |          turn the term IDs into floats, these will be converted back into integers in inference, which incurs a\n |          performance hit. For distributed computing it may be desirable to keep the chunks as `numpy.ndarray`.\n |  \n |  update_alpha(self, gammat, rho)\n |      Update parameters for the Dirichlet prior on the per-document topic weights.\n |      \n |      Parameters\n |      ----------\n |      gammat : numpy.ndarray\n |          Previous topic weight parameters.\n |      rho : float\n |          Learning rate.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          Sequence of alpha parameters.\n |  \n |  update_eta(self, lambdat, rho)\n |      Update parameters for the Dirichlet prior on the per-topic word weights.\n |      \n |      Parameters\n |      ----------\n |      lambdat : numpy.ndarray\n |          Previous lambda parameters.\n |      rho : float\n |          Learning rate.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          The updated eta parameters.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  load(fname, *args, **kwargs) from builtins.type\n |      Load a previously saved :class:`gensim.models.ldamodel.LdaModel` from file.\n |      \n |      See Also\n |      --------\n |      :meth:`~gensim.models.ldamodel.LdaModel.save`\n |          Save model.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          Path to the file where the model is stored.\n |      *args\n |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n |      **kwargs\n |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n |      \n |      Examples\n |      --------\n |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n |      \n |      .. sourcecode:: pycon\n |      \n |          >>> from gensim.test.utils import datapath\n |          >>>\n |          >>> fname = datapath(\"lda_3_0_1_model\")\n |          >>> lda = LdaModel.load(fname, mmap='r')\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from gensim.utils.SaveLoad:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n |  \n |  print_topic(self, topicno, topn=10)\n |      Get a single topic as a formatted string.\n |      \n |      Parameters\n |      ----------\n |      topicno : int\n |          Topic id.\n |      topn : int\n |          Number of words from topic that will be used.\n |      \n |      Returns\n |      -------\n |      str\n |          String representation of topic, like '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + ... '.\n |  \n |  print_topics(self, num_topics=20, num_words=10)\n |      Get the most significant topics (alias for `show_topics()` method).\n |      \n |      Parameters\n |      ----------\n |      num_topics : int, optional\n |          The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n |      num_words : int, optional\n |          The number of words to be included per topics (ordered by significance).\n |      \n |      Returns\n |      -------\n |      list of (int, list of (str, float))\n |          Sequence with (topic_id, [(word, value), ... ]).\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}